# -*- coding: utf-8 -*-
"""JaatChampion_2370300004.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wRO7iJJY0LVABXeJHgh6J5vUIXO-epYO
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

lines = open('fra.txt', encoding='utf-8').read().strip().split('\n')

sentence_pairs = [line.split('\t')[:2] for line in lines]
english_sentences, french_sentences = zip(*sentence_pairs)

french_sentences = ['<start> ' + sent + ' <end>' for sent in french_sentences]

#Tokenize the sentences
def tokenize(lang_sentences):
    tokenizer = Tokenizer(filters='')
    tokenizer.fit_on_texts(lang_sentences)
    tensor = tokenizer.texts_to_sequences(lang_sentences)
    tensor = pad_sequences(tensor, padding='post')
    return tensor, tokenizer

input_tensor, inp_tokenizer = tokenize(english_sentences)
target_tensor, targ_tokenizer = tokenize(french_sentences)

# Step 5: Check shapes
print(f"Input Tensor Shape: {input_tensor.shape}")
print(f"Target Tensor Shape: {target_tensor.shape}")

"""* Build Encoder and Decoder using LSTM (Keras) *"""

embedding_dim = 128
lstm_units = 256
vocab_inp_size = len(inp_tokenizer.word_index) + 1
vocab_tar_size = len(targ_tokenizer.word_index) + 1
max_length_input = input_tensor.shape[1]
max_length_target = target_tensor.shape[1]

# ENCODER
encoder_input = Input(shape=(None,))
enc_emb = Embedding(vocab_inp_size, embedding_dim)(encoder_input)
encoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True)(enc_emb)
encoder_states = [state_h, state_c]

# DECODER
decoder_input = Input(shape=(None,))
dec_emb_layer = Embedding(vocab_tar_size, embedding_dim)
dec_emb = dec_emb_layer(decoder_input)
decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)
decoder_output, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
decoder_dense = Dense(vocab_tar_size, activation='softmax')
output = decoder_dense(decoder_output)

model = Model([encoder_input, decoder_input], output)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

model.summary()

decoder_input_data = target_tensor[:, :-1]
decoder_target_data = target_tensor[:, 1:]
decoder_target_data = np.expand_dims(decoder_target_data, -1)

sample_size = 10000

input_tensor_small = input_tensor[:sample_size]
decoder_input_data_small = decoder_input_data[:sample_size]
decoder_target_data_small = decoder_target_data[:sample_size]

history = model.fit(
    [input_tensor_small, decoder_input_data_small],
    decoder_target_data_small,
    batch_size=64,
    epochs=10,
    validation_split=0.2
)

model.save("eng_fra_translation_model.h5")

encoder_model = Model(encoder_input, encoder_states)

decoder_state_input_h = Input(shape=(lstm_units,))
decoder_state_input_c = Input(shape=(lstm_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

dec_emb2 = dec_emb_layer(decoder_input)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(
    dec_emb2, initial_state=decoder_states_inputs
)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2)

decoder_model = Model(
    [decoder_input] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2
)

def decode_sequence(input_seq):

    states_value = encoder_model.predict(input_seq)

    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = targ_tokenizer.word_index['<start>']

    stop_condition = False
    decoded_sentence = ''

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = targ_tokenizer.index_word.get(sampled_token_index, '')

        if (sampled_word == '<end>' or len(decoded_sentence.split()) > max_length_target):
            stop_condition = True
        else:
            decoded_sentence += ' ' + sampled_word

        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

        states_value = [h, c]

    return decoded_sentence.strip()

def evaluate(sentence):
    sentence_seq = inp_tokenizer.texts_to_sequences([sentence])
    sentence_seq = pad_sequences(sentence_seq, maxlen=max_length_input, padding='post')
    return decode_sequence(sentence_seq)

sentence = "hello"
translated = evaluate(sentence.lower())
print(f"ğŸ“ Input   : {sentence}")
print(f"ğŸ” Output  : {translated}")

sentence = "how are you?"
translated = evaluate(sentence.lower())
print(f"ğŸ“ Input   : {sentence}")
print(f"ğŸ” Output  : {translated}")

sentence = "i love you"
translated = evaluate(sentence.lower())
print(f"ğŸ“ Input   : {sentence}")
print(f"ğŸ” Output  : {translated}")

sentence = "thank you"
translated = evaluate(sentence.lower())
print(f"ğŸ“ Input   : {sentence}")
print(f"ğŸ” Output  : {translated}")

sentence = "run"
translated = evaluate(sentence.lower())
print(f"ğŸ“ Input   : {sentence}")
print(f"ğŸ” Output  : {translated}")

sentence = "what is your name?"
translated = evaluate(sentence.lower())
print(f"ğŸ“ Input   : {sentence}")
print(f"ğŸ” Output  : {translated}")

import matplotlib.pyplot as plt

# Plot Training and Validation Loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""ğŸ”¹ Overfitting:
     definition: When a model learns the training data too well, including noise, and performs poorly on new/unseen data.
     observation: There is no strong evidence of overfitting.The validation loss follows the training loss closely & remains stable across          epochs.

 ğŸ”¹ Underfitting:
     definition: When a model is too simple to capture the underlying patterns in the training data, resulting in poor performance.
     observation: : There is no underfitting. The training loss decreases rapidly, and the validation loss shows improvement too.

 ğŸ”¹ Training Stability:
      definition: The smooth and consistent behavior of the modelâ€™s learning process across epochs without sudden spikes or drops.
      observation: The training is stable as both training and validation loss curves are smooth and show consistent downward trends.

"""





